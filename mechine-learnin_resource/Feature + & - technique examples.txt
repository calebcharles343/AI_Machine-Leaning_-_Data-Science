Feature + & - technique examples

### Feature Reduction Example: Principal Component Analysis (PCA)

PCA is a popular technique for dimensionality reduction. It transforms the data into a lower-dimensional space while retaining most of the variance.

```python
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Sample data
data = pd.DataFrame({
    'feature1': np.random.rand(100),
    'feature2': np.random.rand(100),
    'feature3': np.random.rand(100),
    'feature4': np.random.rand(100)
})

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# Apply PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(scaled_data)

# Create a DataFrame with the principal components
pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
print(pca_df.head())
```

### Feature Addition Example: Polynomial Features

Polynomial features can be created by combining existing features in polynomial terms, which can help capture non-linear relationships.

```python
from sklearn.preprocessing import PolynomialFeatures

# Sample data
data = pd.DataFrame({
    'feature1': np.random.rand(100),
    'feature2': np.random.rand(100)
})

# Create polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(data)

# Create a DataFrame with the polynomial features
poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(data.columns))
print(poly_df.head())
```

### Feature Selection Example: Recursive Feature Elimination (RFE)

RFE is a wrapper method that recursively removes the least important features based on the model performance.

```python
from sklearn.datasets import load_iris
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Load sample data
iris = load_iris()
X = iris.data
y = iris.target

# Create a logistic regression model
model = LogisticRegression(max_iter=200)

# Apply RFE
rfe = RFE(model, n_features_to_select=2)
fit = rfe.fit(X, y)

# Selected features
print("Num Features: %s" % (fit.n_features_))
print("Selected Features: %s" % (fit.support_))
print("Feature Ranking: %s" % (fit.ranking_))
```

These examples should give you a good starting point for implementing feature reduction and addition techniques in your machine learning projects. If you have any specific questions or need further clarification, feel free to ask!